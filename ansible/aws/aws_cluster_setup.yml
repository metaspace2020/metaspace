---

- name: SM spark master initial setup (slaves file update)
  hosts: master
  user: ubuntu
  gather_facts: true

  vars:
    spark_master_host: "spark://{{ ansible_hostname }}:7077"
    spark_slave_ips: []

  tasks:
    - name: Create a list of private ip addresses for the slave instances
      set_fact: spark_slave_ips="{{ spark_slave_ips  + [ hostvars[item].ansible_ssh_host ] }}"
      with_items: "{{ groups['slave'] }}"

    - name: Put slave ip addresses into the slaves file
      become: yes
      template: src=../roles/spark_master/templates/slaves.j2 dest="{{ spark_home }}/conf/slaves"
                owner=ubuntu group=ubuntu mode=0664

    - name: Restart the master and slave daemons
      command: "{{ spark_home }}/sbin/start-all.sh"
      when: not ('localhost' in spark_slave_ips)
      register: command_result
      changed_when: not ('Stop it first' in command_result.stdout)
      failed_when: ('timed out' in command_result.stdout)

    - debug: var=command_result

    - name: Check that master daemon is up
      command: jps -l
      become: no
      when: not ('localhost' in spark_slave_ips)
      register: command_result
      failed_when: not ('Master' in command_result.stdout)

    - name: Wait for slave deamons to start up
      wait_for: host={{ item }} port=8081 delay=5
      with_items: "{{ spark_slave_ips }}"
      when: not ('localhost' in spark_slave_ips)

    - name: Pull SM config from the remote host
      fetch: src={{ sm_home }}/conf/config.json.template dest=/tmp/config.json.template
             flat=yes fail_on_missing=yes

    - name: "Put SM config to {{ sm_home }}/conf/config.json"
      template: src=/tmp/config.json.template dest={{ sm_home }}/conf/config.json
                owner={{ spark_user }} group={{ spark_user }} mode=0600
